"""
Experiment 9: Convergence Rate Analysis

Measures and compares convergence rates between QILTR and Euclidean-LTR
to validate theoretical convergence claims.

Measures:
1. Loss trajectory over ALS iterations
2. Iterations to reach convergence threshold
3. Convergence rate estimation
4. Condition number analysis
"""

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import time
import sys
from pathlib import Path
from scipy.optimize import curve_fit
from scipy.stats import ttest_ind

sys.path.append(str(Path(__file__).parent.parent))

from src.qiltr import QILTR
from src.baselines import EuclideanLTR, GlobalTuckerRegression
from src.synthetic_data import generate_synthetic_tensor_regression
from src.metrics import compute_all_metrics
from config import RANDOM_SEED, RESULTS_DIR

plt.style.use('seaborn-v0_8-paper')
sns.set_palette("husl")


def exponential_decay(iteration, L0, rate, L_inf):
    """
    Exponential convergence model: L(t) = L_inf + (L0 - L_inf) * exp(-rate * t)
    
    Args:
        iteration: Iteration number
        L0: Initial loss
        rate: Convergence rate (higher = faster)
        L_inf: Asymptotic loss
    """
    return L_inf + (L0 - L_inf) * np.exp(-rate * iteration)


def fit_convergence_rate(iterations, losses):
    """
    Fit exponential decay model to loss trajectory.
    
    Returns:
        rate: Convergence rate parameter
        fitted_curve: Fitted loss values
        r_squared: Goodness of fit
    """
    try:
        # Initial parameter guess
        L0_guess = losses[0]
        L_inf_guess = losses[-1]
        rate_guess = 0.1
        
        # Fit exponential decay
        params, _ = curve_fit(
            exponential_decay,
            iterations,
            losses,
            p0=[L0_guess, rate_guess, L_inf_guess],
            maxfev=10000,
            bounds=([0, 0, 0], [np.inf, 1.0, np.inf])
        )
        
        L0_fit, rate_fit, L_inf_fit = params
        fitted_curve = exponential_decay(iterations, L0_fit, rate_fit, L_inf_fit)
        
        # Compute RÂ²
        ss_res = np.sum((losses - fitted_curve) ** 2)
        ss_tot = np.sum((losses - np.mean(losses)) ** 2)
        r_squared = 1 - (ss_res / ss_tot)
        
        return rate_fit, fitted_curve, r_squared
        
    except Exception as e:
        print(f"    Warning: Could not fit convergence rate: {e}")
        return None, None, None


class ConvergenceTrackingQILTR(QILTR):
    """QILTR with convergence tracking for each centroid."""
    
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.centroid_convergence_histories = {}
    
    def fit(self, X, Y):
        """Fit with detailed convergence tracking."""
        N = X.shape[0]
        
        # Select centroids
        self.centroids = self._select_centroids(X)
        
        # Encode centroids to quantum states
        self.centroid_states = self.encoder.encode_batch(self.centroids)
        
        # Encode all data points
        data_states = self.encoder.encode_batch(X)
        
        # Fit local model at each centroid (with tracking)
        from src.distances import bures_distance_batch, compute_weights
        from src.als_solver import WeightedTuckerALS
        
        for c_idx in range(self.n_centroids):
            # Compute distances and weights
            distances = bures_distance_batch(
                self.centroid_states[c_idx], 
                data_states
            )
            weights = compute_weights(distances, self.bandwidth)
            
            # Fit weighted Tucker-ALS with tracking
            als_solver = WeightedTuckerALS(
                ranks=self.ranks,
                max_iter=self.max_als_iter,
                tol=self.als_tol,
                reg_lambda=self.reg_lambda
            )
            
            core, factors = als_solver.fit(Y, weights)
            
            # Store model
            self.local_models[c_idx] = {
                'core': core,
                'factors': factors,
                'centroid': self.centroids[c_idx],
                'centroid_state': self.centroid_states[c_idx]
            }
            
            # Store convergence history
            self.centroid_convergence_histories[c_idx] = als_solver.convergence_history
        
        return self


class ConvergenceTrackingEuclidean(EuclideanLTR):
    """Euclidean-LTR with convergence tracking."""
    
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.centroid_convergence_histories = {}
    
    def fit(self, X, Y):
        """Fit with detailed convergence tracking."""
        N = X.shape[0]
        
        # Select centroids
        from sklearn.cluster import KMeans
        if self.centroid_method == 'kmeans':
            kmeans = KMeans(n_clusters=self.n_centroids, 
                          random_state=self.random_state, n_init=10)
            kmeans.fit(X)
            self.centroids = kmeans.cluster_centers_
        else:
            # Random selection
            np.random.seed(self.random_state)
            indices = np.random.choice(N, self.n_centroids, replace=False)
            self.centroids = X[indices]
        
        # Fit local model at each centroid (with tracking)
        from src.distances import euclidean_distance_batch, compute_weights
        from src.als_solver import WeightedTuckerALS
        
        for c_idx in range(self.n_centroids):
            # Compute Euclidean distances and weights
            distances = euclidean_distance_batch(
                self.centroids[c_idx], 
                X
            )
            weights = compute_weights(distances, self.bandwidth)
            
            # Fit weighted Tucker-ALS with tracking
            als_solver = WeightedTuckerALS(
                ranks=self.ranks,
                max_iter=self.max_als_iter,
                tol=self.als_tol,
                reg_lambda=self.reg_lambda
            )
            
            core, factors = als_solver.fit(Y, weights)
            
            # Store model
            self.local_models[c_idx] = {
                'core': core,
                'factors': factors,
                'centroid': self.centroids[c_idx]
            }
            
            # Store convergence history
            self.centroid_convergence_histories[c_idx] = als_solver.convergence_history
        
        return self


def analyze_convergence_single_trial(scenario_name, scenario_config, trial_seed, base_params):
    """
    Analyze convergence for a single trial.
    
    Returns:
        Dictionary with convergence metrics for QILTR and Euclidean
    """
    print(f"\n  Trial seed={trial_seed}")
    
    # Generate challenging data that requires multiple iterations
    X, Y, region_labels, true_tensors = generate_synthetic_tensor_regression(
        n_samples=1500,  # More samples for better statistics
        input_dim=25,  # Higher dimensional input
        tensor_shape=(6, 6, 6),  # Larger tensors
        tucker_ranks=(4, 4, 4),  # Higher true rank
        n_regions=scenario_config.get('n_regions', 5),
        noise_level=scenario_config.get('noise_level', 0.3),
        random_state=trial_seed,
        high_nonstationarity=scenario_config.get('high_nonstationarity', True),
        ill_conditioned=scenario_config.get('ill_conditioned', True)
    )
    
    from sklearn.model_selection import train_test_split
    X_train, X_test, Y_train, Y_test = train_test_split(
        X, Y, test_size=0.2, random_state=trial_seed
    )
    
    results = {}
    
    # Test QILTR
    print("    QILTR...", end=' ')
    qiltr = ConvergenceTrackingQILTR(**base_params, random_state=trial_seed)
    qiltr.fit(X_train, Y_train)
    
    # Aggregate convergence histories across centroids
    all_iterations = []
    all_losses = []
    for c_idx in range(qiltr.n_centroids):
        history = qiltr.centroid_convergence_histories[c_idx]
        all_iterations.extend(list(range(len(history))))
        all_losses.extend(history)
    
    # Average convergence trajectory
    max_iters = max(len(qiltr.centroid_convergence_histories[c]) 
                   for c in range(qiltr.n_centroids))
    avg_trajectory = []
    for it in range(max_iters):
        losses_at_iter = [qiltr.centroid_convergence_histories[c][it] 
                         for c in range(qiltr.n_centroids) 
                         if it < len(qiltr.centroid_convergence_histories[c])]
        avg_trajectory.append(np.mean(losses_at_iter))
    
    # Fit convergence rate
    iterations_arr = np.arange(len(avg_trajectory))
    rate, fitted, r2 = fit_convergence_rate(iterations_arr, np.array(avg_trajectory))
    
    # Iterations to convergence (90% of final improvement)
    if len(avg_trajectory) > 1:
        initial_loss = avg_trajectory[0]
        final_loss = avg_trajectory[-1]
        threshold = final_loss + 0.1 * (initial_loss - final_loss)
        iters_to_convergence = next((i for i, loss in enumerate(avg_trajectory) 
                                    if loss <= threshold), len(avg_trajectory))
    else:
        iters_to_convergence = 1
    
    # Final performance
    Y_pred = qiltr.predict(X_test)
    final_metrics = compute_all_metrics(Y_test, Y_pred)
    
    results['QILTR'] = {
        'avg_trajectory': avg_trajectory,
        'convergence_rate': rate if rate is not None else 0.0,
        'fitted_curve': fitted,
        'r2_fit': r2 if r2 is not None else 0.0,
        'iters_to_convergence': iters_to_convergence,
        'final_mse': final_metrics['mse'],
        'total_iterations': len(avg_trajectory)
    }
    
    rate_str = f"{rate:.4f}" if rate is not None else "0.0000"
    print(f"rate={rate_str}, iters={iters_to_convergence}, final_mse={final_metrics['mse']:.4f}")
    
    # Test Euclidean-LTR
    print("    Euclidean-LTR...", end=' ')
    euclidean = ConvergenceTrackingEuclidean(**base_params, random_state=trial_seed)
    euclidean.fit(X_train, Y_train)
    
    # Aggregate convergence histories
    max_iters_euc = max(len(euclidean.centroid_convergence_histories[c]) 
                       for c in range(euclidean.n_centroids))
    avg_trajectory_euc = []
    for it in range(max_iters_euc):
        losses_at_iter = [euclidean.centroid_convergence_histories[c][it] 
                         for c in range(euclidean.n_centroids) 
                         if it < len(euclidean.centroid_convergence_histories[c])]
        avg_trajectory_euc.append(np.mean(losses_at_iter))
    
    # Fit convergence rate
    iterations_arr_euc = np.arange(len(avg_trajectory_euc))
    rate_euc, fitted_euc, r2_euc = fit_convergence_rate(iterations_arr_euc, np.array(avg_trajectory_euc))
    
    # Iterations to convergence
    if len(avg_trajectory_euc) > 1:
        initial_loss_euc = avg_trajectory_euc[0]
        final_loss_euc = avg_trajectory_euc[-1]
        threshold_euc = final_loss_euc + 0.1 * (initial_loss_euc - final_loss_euc)
        iters_to_convergence_euc = next((i for i, loss in enumerate(avg_trajectory_euc) 
                                        if loss <= threshold_euc), len(avg_trajectory_euc))
    else:
        iters_to_convergence_euc = 1
    
    # Final performance
    Y_pred_euc = euclidean.predict(X_test)
    final_metrics_euc = compute_all_metrics(Y_test, Y_pred_euc)
    
    results['Euclidean'] = {
        'avg_trajectory': avg_trajectory_euc,
        'convergence_rate': rate_euc if rate_euc is not None else 0.0,
        'fitted_curve': fitted_euc,
        'r2_fit': r2_euc if r2_euc is not None else 0.0,
        'iters_to_convergence': iters_to_convergence_euc,
        'final_mse': final_metrics_euc['mse'],
        'total_iterations': len(avg_trajectory_euc)
    }
    
    rate_str_euc = f"{rate_euc:.4f}" if rate_euc is not None else "0.0000"
    print(f"rate={rate_str_euc}, iters={iters_to_convergence_euc}, final_mse={final_metrics_euc['mse']:.4f}")
    
    return results


def run_convergence_analysis(n_trials=10):
    """
    Run comprehensive convergence analysis.
    
    Args:
        n_trials: Number of independent trials
    """
    print("="*80)
    print("EXPERIMENT 9: CONVERGENCE RATE ANALYSIS")
    print("="*80)
    print(f"\nRunning {n_trials} independent trials to measure:")
    print("  1. Loss trajectory over ALS iterations")
    print("  2. Convergence rate (exponential decay constant)")
    print("  3. Iterations to reach 90% convergence")
    print("  4. Final prediction performance")
    print("\n" + "="*80)
    
    # Test on ill-conditioned scenario (forces more ALS iterations)
    # This creates a harder optimization problem where convergence behavior differs
    scenario_config = {
        'n_regions': 5,  # More regions = more complexity
        'noise_level': 0.3,  # Lower noise to see convergence dynamics
        'high_nonstationarity': True,  # High variation between regions
        'ill_conditioned': True  # Poorly conditioned tensors force more iterations
    }
    
    base_params = {
        'n_centroids': 15,  # More centroids for finer granularity
        'bandwidth': 0.5,  # Tighter bandwidth = more local fitting
        'ranks': (4, 4, 4),  # Slightly higher rank for harder optimization
        'max_als_iter': 100,
        'als_tol': 1e-8,  # Stricter tolerance to see full convergence curve
        'reg_lambda': 0.001  # Small regularization for numerical stability
    }
    
    print(f"\nScenario: Ill-Conditioned (challenging convergence scenario)")
    print(f"Parameters: {base_params}")
    print("Note: Using ill-conditioned tensors to force multiple ALS iterations\n")
    
    # Collect results across trials
    all_results = {
        'QILTR': {
            'convergence_rates': [],
            'iters_to_convergence': [],
            'final_mse': [],
            'trajectories': []
        },
        'Euclidean': {
            'convergence_rates': [],
            'iters_to_convergence': [],
            'final_mse': [],
            'trajectories': []
        }
    }
    
    for trial in range(n_trials):
        trial_seed = RANDOM_SEED + trial
        trial_results = analyze_convergence_single_trial(
            'standard', scenario_config, trial_seed, base_params
        )
        
        # Store QILTR results
        all_results['QILTR']['convergence_rates'].append(
            trial_results['QILTR']['convergence_rate']
        )
        all_results['QILTR']['iters_to_convergence'].append(
            trial_results['QILTR']['iters_to_convergence']
        )
        all_results['QILTR']['final_mse'].append(
            trial_results['QILTR']['final_mse']
        )
        all_results['QILTR']['trajectories'].append(
            trial_results['QILTR']['avg_trajectory']
        )
        
        # Store Euclidean results
        all_results['Euclidean']['convergence_rates'].append(
            trial_results['Euclidean']['convergence_rate']
        )
        all_results['Euclidean']['iters_to_convergence'].append(
            trial_results['Euclidean']['iters_to_convergence']
        )
        all_results['Euclidean']['final_mse'].append(
            trial_results['Euclidean']['final_mse']
        )
        all_results['Euclidean']['trajectories'].append(
            trial_results['Euclidean']['avg_trajectory']
        )
    
    return all_results


def generate_convergence_report(results, output_dir):
    """Generate comprehensive convergence analysis report."""
    output_dir = Path(output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # Statistical comparison
    qiltr_rates = np.array(results['QILTR']['convergence_rates'])
    euc_rates = np.array(results['Euclidean']['convergence_rates'])
    
    qiltr_iters = np.array(results['QILTR']['iters_to_convergence'])
    euc_iters = np.array(results['Euclidean']['iters_to_convergence'])
    
    qiltr_mse = np.array(results['QILTR']['final_mse'])
    euc_mse = np.array(results['Euclidean']['final_mse'])
    
    # T-tests
    rate_ttest = ttest_ind(qiltr_rates, euc_rates)
    iters_ttest = ttest_ind(qiltr_iters, euc_iters)
    mse_ttest = ttest_ind(qiltr_mse, euc_mse)
    
    # Generate report
    report_path = output_dir / 'convergence_analysis_report.md'
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write("# Convergence Rate Analysis Report\n\n")
        f.write("**Empirical Validation of Theorem 1: Convergence Rate Claims**\n\n")
        f.write("This experiment directly measures convergence properties to validate\n")
        f.write("theoretical claims about QILTR's superior convergence rate.\n\n")
        f.write("---\n\n")
        
        f.write("## Methodology\n\n")
        f.write(f"- **Trials**: {len(qiltr_rates)} independent runs\n")
        f.write("- **Scenario**: Standard (clean, non-adversarial setting)\n")
        f.write("- **Metrics**:\n")
        f.write("  1. Convergence rate (exponential decay parameter)\n")
        f.write("  2. Iterations to 90% convergence\n")
        f.write("  3. Final prediction MSE\n\n")
        f.write("---\n\n")
        
        f.write("## Results Summary\n\n")
        
        # Convergence Rate
        f.write("### 1. Convergence Rate (Higher = Faster)\n\n")
        f.write("| Method | Mean Rate | Std Rate | Median Rate |\n")
        f.write("|--------|-----------|----------|-----------|\n")
        f.write(f"| QILTR | {np.mean(qiltr_rates):.6f} | {np.std(qiltr_rates):.6f} | {np.median(qiltr_rates):.6f} |\n")
        f.write(f"| Euclidean-LTR | {np.mean(euc_rates):.6f} | {np.std(euc_rates):.6f} | {np.median(euc_rates):.6f} |\n\n")
        
        rate_improvement = ((np.mean(qiltr_rates) - np.mean(euc_rates)) / np.mean(euc_rates)) * 100
        f.write(f"**Rate Comparison**: QILTR is {rate_improvement:+.2f}% ")
        f.write("faster\n" if rate_improvement > 0 else "slower\n")
        f.write(f"**Statistical Significance**: t-statistic={rate_ttest.statistic:.4f}, ")
        f.write(f"p-value={rate_ttest.pvalue:.6f}\n\n")
        
        if rate_ttest.pvalue < 0.05:
            f.write("â **Statistically significant difference** (p < 0.05)\n\n")
        else:
            f.write("â ï¸ **No statistically significant difference** (p >= 0.05)\n\n")
        
        # Iterations to Convergence
        f.write("### 2. Iterations to 90% Convergence (Lower = Faster)\n\n")
        f.write("| Method | Mean Iters | Std Iters | Median Iters |\n")
        f.write("|--------|------------|-----------|-------------|\n")
        f.write(f"| QILTR | {np.mean(qiltr_iters):.2f} | {np.std(qiltr_iters):.2f} | {np.median(qiltr_iters):.0f} |\n")
        f.write(f"| Euclidean-LTR | {np.mean(euc_iters):.2f} | {np.std(euc_iters):.2f} | {np.median(euc_iters):.0f} |\n\n")
        
        iter_improvement = ((np.mean(euc_iters) - np.mean(qiltr_iters)) / np.mean(euc_iters)) * 100
        f.write(f"**Iteration Reduction**: QILTR converges {iter_improvement:+.2f}% ")
        f.write("faster\n" if iter_improvement > 0 else "slower\n")
        f.write(f"**Statistical Significance**: t-statistic={iters_ttest.statistic:.4f}, ")
        f.write(f"p-value={iters_ttest.pvalue:.6f}\n\n")
        
        if iters_ttest.pvalue < 0.05:
            f.write("â **Statistically significant difference** (p < 0.05)\n\n")
        else:
            f.write("â ï¸ **No statistically significant difference** (p >= 0.05)\n\n")
        
        # Final MSE
        f.write("### 3. Final Prediction Performance\n\n")
        f.write("| Method | Mean MSE | Std MSE | Median MSE |\n")
        f.write("|--------|----------|---------|------------|\n")
        f.write(f"| QILTR | {np.mean(qiltr_mse):.6f} | {np.std(qiltr_mse):.6f} | {np.median(qiltr_mse):.6f} |\n")
        f.write(f"| Euclidean-LTR | {np.mean(euc_mse):.6f} | {np.std(euc_mse):.6f} | {np.median(euc_mse):.6f} |\n\n")
        
        mse_improvement = ((np.mean(euc_mse) - np.mean(qiltr_mse)) / np.mean(euc_mse)) * 100
        f.write(f"**MSE Improvement**: {mse_improvement:+.2f}%\n")
        f.write(f"**Statistical Significance**: t-statistic={mse_ttest.statistic:.4f}, ")
        f.write(f"p-value={mse_ttest.pvalue:.6f}\n\n")
        
        if mse_ttest.pvalue < 0.05:
            f.write("â **Statistically significant difference** (p < 0.05)\n\n")
        else:
            f.write("â ï¸ **No statistically significant difference** (p >= 0.05)\n\n")
        
        # Overall Verdict
        f.write("---\n\n")
        f.write("## Verdict on Theorem 1 Claims\n\n")
        
        f.write("**Claim**: QILTR exhibits superior convergence rate due to geometric weighting\n\n")
        
        if rate_improvement > 0 and rate_ttest.pvalue < 0.05:
            f.write("â **SUPPORTED**: QILTR shows statistically significant faster convergence\n\n")
        elif rate_improvement > 0 and rate_ttest.pvalue >= 0.05:
            f.write("ð¡ **PARTIALLY SUPPORTED**: QILTR trends faster but not statistically significant\n\n")
        else:
            f.write("â **NOT SUPPORTED**: Euclidean-LTR converges as fast or faster\n\n")
        
        f.write("**Interpretation**:\n\n")
        if rate_improvement > 5 and rate_ttest.pvalue < 0.05:
            f.write("The empirical evidence strongly supports Theorem 1's claims. QILTR's geometric\n")
            f.write("weighting scheme demonstrably accelerates convergence in practice.\n\n")
        elif abs(rate_improvement) < 5:
            f.write("Convergence rates are statistically similar between methods. The theoretical\n")
            f.write("advantage of geometric weighting may be modest in practice, or the benefits\n")
            f.write("may only manifest in specific problem structures.\n\n")
        else:
            f.write("Unexpectedly, Euclidean-LTR shows faster convergence. This suggests that\n")
            f.write("Theorem 1's assumptions may not hold for this problem class, or that other\n")
            f.write("factors (like centroid selection) dominate convergence behavior.\n\n")
        
        f.write("---\n\n")
        f.write("**Recommendation for Manuscript**:\n\n")
        if rate_improvement > 5 and rate_ttest.pvalue < 0.05:
            f.write("Include these results as strong empirical validation of convergence claims.\n")
        else:
            f.write("Acknowledge that empirical convergence rates are similar, and emphasize\n")
            f.write("that the primary benefit is in final solution quality rather than convergence speed.\n")
    
    print(f"\nâ Saved report: {report_path}")
    
    return report_path


def plot_convergence_curves(results, output_dir):
    """Generate convergence curve visualizations."""
    output_dir = Path(output_dir)
    figures_dir = output_dir / 'figures'
    figures_dir.mkdir(parents=True, exist_ok=True)
    
    # Figure 1: Average convergence trajectories
    fig, axes = plt.subplots(1, 2, figsize=(14, 5))
    
    # Plot individual trials (faded)
    for traj in results['QILTR']['trajectories']:
        axes[0].plot(traj, alpha=0.2, color='blue', linewidth=0.5)
    for traj in results['Euclidean']['trajectories']:
        axes[1].plot(traj, alpha=0.2, color='orange', linewidth=0.5)
    
    # Plot mean trajectory (bold)
    max_len_q = max(len(t) for t in results['QILTR']['trajectories'])
    max_len_e = max(len(t) for t in results['Euclidean']['trajectories'])
    
    mean_traj_q = []
    std_traj_q = []
    for i in range(max_len_q):
        values = [t[i] for t in results['QILTR']['trajectories'] if i < len(t)]
        mean_traj_q.append(np.mean(values))
        std_traj_q.append(np.std(values))
    
    mean_traj_e = []
    std_traj_e = []
    for i in range(max_len_e):
        values = [t[i] for t in results['Euclidean']['trajectories'] if i < len(t)]
        mean_traj_e.append(np.mean(values))
        std_traj_e.append(np.std(values))
    
    axes[0].plot(mean_traj_q, color='blue', linewidth=2, label='Mean trajectory')
    axes[0].fill_between(range(len(mean_traj_q)), 
                         np.array(mean_traj_q) - np.array(std_traj_q),
                         np.array(mean_traj_q) + np.array(std_traj_q),
                         alpha=0.3, color='blue')
    
    axes[1].plot(mean_traj_e, color='orange', linewidth=2, label='Mean trajectory')
    axes[1].fill_between(range(len(mean_traj_e)), 
                         np.array(mean_traj_e) - np.array(std_traj_e),
                         np.array(mean_traj_e) + np.array(std_traj_e),
                         alpha=0.3, color='orange')
    
    axes[0].set_title('QILTR Convergence', fontsize=14, fontweight='bold')
    axes[0].set_xlabel('ALS Iteration', fontsize=12)
    axes[0].set_ylabel('Loss (Reconstruction Error)', fontsize=12)
    axes[0].set_yscale('log')
    axes[0].grid(True, alpha=0.3)
    axes[0].legend()
    
    axes[1].set_title('Euclidean-LTR Convergence', fontsize=14, fontweight='bold')
    axes[1].set_xlabel('ALS Iteration', fontsize=12)
    axes[1].set_ylabel('Loss (Reconstruction Error)', fontsize=12)
    axes[1].set_yscale('log')
    axes[1].grid(True, alpha=0.3)
    axes[1].legend()
    
    plt.tight_layout()
    fig_path = figures_dir / 'convergence_trajectories.png'
    plt.savefig(fig_path, dpi=300, bbox_inches='tight')
    plt.close()
    print(f"â Saved: {fig_path}")
    
    # Figure 2: Direct comparison
    fig, axes = plt.subplots(1, 3, figsize=(15, 4))
    
    # Convergence rate comparison
    data_rates = pd.DataFrame({
        'QILTR': results['QILTR']['convergence_rates'],
        'Euclidean-LTR': results['Euclidean']['convergence_rates']
    })
    data_rates.plot(kind='box', ax=axes[0])
    axes[0].set_title('Convergence Rate\n(Higher = Faster)', fontsize=12, fontweight='bold')
    axes[0].set_ylabel('Exponential Decay Rate', fontsize=11)
    axes[0].grid(True, alpha=0.3, axis='y')
    
    # Iterations to convergence
    data_iters = pd.DataFrame({
        'QILTR': results['QILTR']['iters_to_convergence'],
        'Euclidean-LTR': results['Euclidean']['iters_to_convergence']
    })
    data_iters.plot(kind='box', ax=axes[1])
    axes[1].set_title('Iterations to 90% Convergence\n(Lower = Faster)', fontsize=12, fontweight='bold')
    axes[1].set_ylabel('Iteration Count', fontsize=11)
    axes[1].grid(True, alpha=0.3, axis='y')
    
    # Final MSE
    data_mse = pd.DataFrame({
        'QILTR': results['QILTR']['final_mse'],
        'Euclidean-LTR': results['Euclidean']['final_mse']
    })
    data_mse.plot(kind='box', ax=axes[2])
    axes[2].set_title('Final Prediction MSE\n(Lower = Better)', fontsize=12, fontweight='bold')
    axes[2].set_ylabel('Mean Squared Error', fontsize=11)
    axes[2].grid(True, alpha=0.3, axis='y')
    
    plt.tight_layout()
    fig_path = figures_dir / 'convergence_comparison.png'
    plt.savefig(fig_path, dpi=300, bbox_inches='tight')
    plt.close()
    print(f"â Saved: {fig_path}")
    
    # Figure 3: Overlay comparison
    fig, ax = plt.subplots(1, 1, figsize=(10, 6))
    
    ax.plot(mean_traj_q, color='blue', linewidth=2.5, label='QILTR', alpha=0.8)
    ax.fill_between(range(len(mean_traj_q)), 
                    np.array(mean_traj_q) - np.array(std_traj_q),
                    np.array(mean_traj_q) + np.array(std_traj_q),
                    alpha=0.2, color='blue')
    
    ax.plot(mean_traj_e, color='orange', linewidth=2.5, label='Euclidean-LTR', alpha=0.8)
    ax.fill_between(range(len(mean_traj_e)), 
                    np.array(mean_traj_e) - np.array(std_traj_e),
                    np.array(mean_traj_e) + np.array(std_traj_e),
                    alpha=0.2, color='orange')
    
    ax.set_title('Convergence Rate Comparison: QILTR vs Euclidean-LTR', 
                fontsize=14, fontweight='bold')
    ax.set_xlabel('ALS Iteration', fontsize=12)
    ax.set_ylabel('Loss (Reconstruction Error, log scale)', fontsize=12)
    ax.set_yscale('log')
    ax.grid(True, alpha=0.3)
    ax.legend(fontsize=11)
    
    plt.tight_layout()
    fig_path = figures_dir / 'convergence_overlay.png'
    plt.savefig(fig_path, dpi=300, bbox_inches='tight')
    plt.close()
    print(f"â Saved: {fig_path}")


def main():
    """Run convergence analysis experiment."""
    
    # Run analysis
    results = run_convergence_analysis(n_trials=10)
    
    # Generate visualizations
    print("\n" + "="*80)
    print("GENERATING VISUALIZATIONS")
    print("="*80)
    plot_convergence_curves(results, RESULTS_DIR)
    
    # Generate report
    print("\n" + "="*80)
    print("GENERATING REPORT")
    print("="*80)
    report_path = generate_convergence_report(results, RESULTS_DIR)
    
    print("\n" + "="*80)
    print("CONVERGENCE ANALYSIS COMPLETE")
    print("="*80)
    print(f"\nð Results saved to: {RESULTS_DIR}")
    print(f"ð Report: {report_path}")
    print("\nKey Findings:")
    
    qiltr_rate = np.mean(results['QILTR']['convergence_rates'])
    euc_rate = np.mean(results['Euclidean']['convergence_rates'])
    rate_diff = ((qiltr_rate - euc_rate) / euc_rate) * 100
    
    qiltr_iters = np.mean(results['QILTR']['iters_to_convergence'])
    euc_iters = np.mean(results['Euclidean']['iters_to_convergence'])
    iter_diff = ((euc_iters - qiltr_iters) / euc_iters) * 100
    
    print(f"  Convergence Rate: QILTR {rate_diff:+.2f}% vs Euclidean")
    print(f"  Iterations to 90%: QILTR {iter_diff:+.2f}% faster")
    
    # Statistical test
    from scipy.stats import ttest_ind
    _, p_value = ttest_ind(results['QILTR']['convergence_rates'], 
                          results['Euclidean']['convergence_rates'])
    
    if p_value < 0.05:
        print(f"  Statistical significance: p={p_value:.6f} â SIGNIFICANT")
    else:
        print(f"  Statistical significance: p={p_value:.6f} â ï¸ NOT SIGNIFICANT")


if __name__ == '__main__':
    main()
